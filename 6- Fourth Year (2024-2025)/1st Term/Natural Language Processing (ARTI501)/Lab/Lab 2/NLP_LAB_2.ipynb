{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX3628bqchXH"
      },
      "source": [
        "#Lab2: Text Pre-processing and Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_QqV0Ixcoqx"
      },
      "source": [
        " In this lab, you will learn essential techniques for preparing and cleaning text data using various pre-processing steps. Additionally, you will gain hands-on experience with regular expressions, a powerful tool for pattern matching and text manipulation in natural language processing (NLP) tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7TJ6o9RfguM"
      },
      "source": [
        "_____________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dp76oF3-NR5"
      },
      "source": [
        "## Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnlnllKY-NR6"
      },
      "source": [
        "Regular expressions (regex) are extremely useful in extracting information from any text by searching for one or more matches of a specific search pattern\n",
        "\n",
        "https://www.regexpal.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtru04nV-NR7"
      },
      "source": [
        "#### Regular Expressions use cases\n",
        "\n",
        "- Data pre-processing\n",
        "- Pattern matching\n",
        "- Text feature Engineering\n",
        "- Web scraping\n",
        "- Data validation\n",
        "- Data extraction\n",
        "\n",
        "An example use case is extracting all hashtags from a tweet, or getting email addresses or phone numbers from large unstructured text content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXZMdUMF-NR8"
      },
      "source": [
        "## Regular Expressions with Python\n",
        "\n",
        "Python provides a convenient built-in module for managing regular expressions:\n",
        "\n",
        "> import re\n",
        "\n",
        "We can import it as we import any other Python library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ITtkEZ-NR9"
      },
      "source": [
        "### Important symbols in Regular Expression\n",
        "\n",
        "Let’s start with the basic regular expression characters and some examples:\n",
        "\n",
        "- (^) : Matches the expression to its right, at the start of a string before it finds a line break.\n",
        "- ($) : Matches the expression to its left, at the end of a string before it finds a line break.\n",
        "- (.) : Matches any character except newline.\n",
        "- (a) : Matches exactly one character.\n",
        "- (ab) : Matches the string ab.\n",
        "\n",
        "----------------------------------------------------------------------------\n",
        "Quantifiers:\n",
        "\n",
        "- (a|b) : Matches expression a or b. If a is matched first, b is not checked;\n",
        "- (+) : Matches the expression to its left 1 or more times;\n",
        "- (*) : Matches the expression to its left 0 or more times;\n",
        "- (?) : Matches the expression to its left 0 or 1 times.\n",
        "----------------------------------------------------------------------------\n",
        "Character Classes:\n",
        "\n",
        "- \\w : Matches alphanumeric characters, that is a-z, A-Z, 0–9, and underscore(_);\n",
        "- \\W : Matches non-alphanumeric characters, that is except a-z, A-Z, 0–9, and _;\n",
        "- \\d : Matches digits, from 0–9;\n",
        "- \\D : Matches any non-digits;\n",
        "- \\s : Matches whitespace characters, which also include the \\t, \\n, \\r, and space characters;\n",
        "- \\S : Matches non-whitespace characters.\n",
        "- \\n : Matches a newline character;\n",
        "- \\t : Matches a tab character;\n",
        "- \\b : Matches the word boundary (or empty string) at the start and end of a word;\n",
        "- \\B : Matches where \\b does not, that is non-word boundary.\n",
        "\n",
        "----------------------------------------------------------------------------\n",
        "Sets:\n",
        "\n",
        "- [abc] : Matches either a, b, or c. It does not match abc;\n",
        "- [a-z] : Matches any alphabet from a to z;\n",
        "- [A-Z] : Matches any alphabets in capital from A to Z;\n",
        "- [a\\-p] : Matches a, -, or p. It matches - because \\ escapes it;\n",
        "- [-z] : Matches - or z;\n",
        "- [a-z0–9] : Matches characters from a to z or from 0 to 9.\n",
        "- [(+*)] : Special characters become literal inside a set, so this matches (, +, *, or );\n",
        "- [^ab5] : Adding ^ excludes any character in the set. Here, it matches characters that are not a, b, or 5;\n",
        "----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "### Regular Expression functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx0ibSuOIm6q"
      },
      "source": [
        "#### search\n",
        "\n",
        "Search for pattern occurrences in a string using the search function of the **re module**. This function returns a match object, containing the matched substring (or None, if it doesn’t exist) and its position inside the original string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dXW7Ylso-NR_",
        "outputId": "6edf6c95-345e-464b-b522-2619d097bc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 1), match='I'>\n",
            "<re.Match object; span=(26, 29), match='se.'>\n",
            "<re.Match object; span=(2, 4), match='am'>\n",
            "<re.Match object; span=(3, 4), match='m'>\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = 'I am enjoying the NLP course.'\n",
        "\n",
        "print(re.search(r\"I\", text)) #r is maning i will start regular\n",
        "print(re.search(r\"se.$\", text)) #$ look form the end\n",
        "print(re.search(r\"am\", text))\n",
        "print(re.search(r\"m\", text))\n",
        "print(re.search(r\"AI\", text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1gfK59D8-NSF",
        "outputId": "e3e3c466-6f35-48e2-9b9b-222758252fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, 1)\n",
            "0\n",
            "1\n",
            "The matched pattern is:  I\n"
          ]
        }
      ],
      "source": [
        "match = re.search((r\"I\"),text)\n",
        "print(match.span())\n",
        "print(match.start())\n",
        "print(match.end())\n",
        "print(\"The matched pattern is: \",text[match.start():match.end()]) #index from text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fgvBMaX-NSG"
      },
      "source": [
        "#### match\n",
        "\n",
        "The match function is similar to search, but it only tries to match the pattern at the beginning of the target string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qVT3w-DW-NSH",
        "outputId": "84cac642-46f6-4407-9173-99c491d42285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 29), match='I am enjoying the NLP course.'>\n",
            "<re.Match object; span=(0, 29), match='I am enjoying the NLP course.'>\n",
            "<re.Match object; span=(5, 29), match='enjoying the NLP course.'>\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = 'I am enjoying the NLP course.'\n",
        "\n",
        "print(re.search(r\"I.*\", text)) #globel\n",
        "print(re.match(r\"I.*\", text)) #local >> see the start word\n",
        "\n",
        "print(re.search(r\"enjoying.*\", text))\n",
        "print(re.match(r\"enjoying.*\", text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdqljP2P-NSI",
        "outputId": "36f6e478-491c-47cc-c046-bec835c7c748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(5, 8), match='was'>\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = '1999 was the year I was born.'\n",
        "\n",
        "print(re.search(r\"[a-zA-Z]+\", text))\n",
        "print(re.match(r\"[a-zA-Z]+\", text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO7_8Kua-NSJ"
      },
      "source": [
        "Search does a global search, whereas match does a local search!\n",
        "\n",
        "**Match** is often used when you need to check if the string starts with a specific pattern.\n",
        "\n",
        "\n",
        "**Search** is used when you want to find a pattern anywhere within the string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEQOYN-S-NSJ"
      },
      "source": [
        "#### findall\n",
        "\n",
        "The findall function looks for all the pattern matches in the target string (whereas search and match look only for the first occurrence). The former returns a list, and the latter returns an iterator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0dKYdXL-NSK",
        "outputId": "189fc6a0-79bb-4aa3-aa92-b4557ec695d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1', '2', '3']\n",
            "['1', '2', '3']\n",
            "['1', '222', '3']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text1 = \"a 1 b 2 c 3\"\n",
        "text2 = \"a1 b2 c 3\"\n",
        "text3 = \"a 1 b 222 c 3\"\n",
        "\n",
        "print(re.findall(r\"\\d+\", text1)) #match any digit\n",
        "print(re.findall(r\"\\d+\", text2))\n",
        "print(re.findall(r\"\\d+\", text3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9A1kc-3-NSK"
      },
      "source": [
        "#### sub\n",
        "\n",
        "The sub is a function that finds patterns in the target string and substitute them with another string.\n",
        "\n",
        "flags=re.I --> non-case-sensitive\n",
        "\n",
        "count = specify number of matches you want to replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yNmRC-Z-NSL",
        "outputId": "caa2786b-806d-4e43-de6c-a1026352f452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am enjoying the NLP course.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = 'I am enjoying the Math course.'\n",
        "\n",
        "print(re.sub(r\"Math\", \"NLP\", text,1,flags=re.I)) #flags=re.I >> not mater if M or m\n",
        "                                                 # 1 >> number of word i need it to change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6UtEXBM-NSL"
      },
      "source": [
        "#### compile\n",
        "\n",
        "The compile function compiles a regular expression into a regular expression object, which allows for caching and faster pattern matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1JOhK37-NSL",
        "outputId": "ae2de2c2-fefb-4749-afd5-07b0515cc178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This year is 2022\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"This year is 2021\"\n",
        "pattern = re.compile(r\"\\d+\")\n",
        "\n",
        "print(pattern.sub(\"2022\", text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4YR_w4w-NSM"
      },
      "source": [
        "#### split\n",
        "\n",
        "The split function is similar to the Python split function, but splits according to regular expression patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyLKLCtV-NSM",
        "outputId": "2323afa2-6b38-4a7b-edcf-753576766e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a ', ' b ', ' c ', '']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"a 1 b 2 c 3\"\n",
        "\n",
        "print(re.split(r\"\\d+\", text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XZ73L2nfd0F"
      },
      "source": [
        "__________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgG3jzku-NSN"
      },
      "source": [
        "## Text pre-processing steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIG0Dgbw-NSN"
      },
      "source": [
        "Text preprocessing involves transforming text into a clean and consistent format that can then be fed into a model for further analysis and learning. Raw text data might contain unwanted or unimportant text due to which our results might not give efficient accuracy, and might make it hard to understand and analyze."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMfxsG5k-NSO"
      },
      "source": [
        "**The various text preprocessing steps are:**\n",
        "\n",
        "1. Tokenization.\n",
        "2. Lower casing.\n",
        "3. Stop words removal.\n",
        "4. Stemming.\n",
        "5. Lemmatization.\n",
        "\n",
        "Before implementing the pre-processing, let us understand the concept first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-mB5Sl7-NSO"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is used in NLP to split paragraphs and sentences into smaller units that can be more easily assigned meaning.\n",
        "\n",
        "The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeQkou6S-NSP"
      },
      "source": [
        "- **Sentance Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvm4ZAdi-NSP",
        "outputId": "4d2cdd77-1c5c-4781-c4b0-ea2e21ecad3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"I'm enjoying the NLP course!\", 'I am also learning new concepts.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "text = \"I'm enjoying the NLP course! I am also learning new concepts.\"\n",
        "\n",
        "print(nltk.sent_tokenize(text)) #base on . >> which is the end of sentencce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIagBR-0-NSQ"
      },
      "source": [
        "- **Word Tokenzitaion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JVLO9wi-NSQ",
        "outputId": "9d2b245e-d0cb-4f06-e958-c1bf1af0636a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"I'm\", 'enjoying', 'the', 'NLP', 'course!']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I'm enjoying the NLP course!\"\n",
        "text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQU3873R-NSR",
        "outputId": "97f92832-97a8-46a7-93cb-2fd135c1b14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', \"'m\", 'enjoying', 'the', 'NLP', 'course', '!']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "text = \"I'm enjoying the NLP course!\"\n",
        "\n",
        "print(nltk.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpAAaORS-NSR"
      },
      "outputs": [],
      "source": [
        "#!pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB5JY2o6-NSS",
        "outputId": "c998a016-6a33-47d6-dba5-cc9a5f1d23ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I\n",
            "'m\n",
            "enjoying\n",
            "the\n",
            "NLP\n",
            "course\n",
            "!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(\"I'm enjoying the NLP course!\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTX_Ty4F-NSS",
        "outputId": "f2cdb352-20a7-4fef-881e-d278d8648123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nltk ['A', '5km', 'NYC', 'cab', 'ride', 'costs', '$', '10.30']\n",
            "##############################\n",
            "A\n",
            "5\n",
            "km\n",
            "NYC\n",
            "cab\n",
            "ride\n",
            "costs\n",
            "$\n",
            "10.30\n"
          ]
        }
      ],
      "source": [
        "text = \"A 5km NYC cab ride costs $10.30\"\n",
        "doc = nlp(\"A 5km NYC cab ride costs $10.30\")\n",
        "print('nltk', nltk.word_tokenize(text))\n",
        "\n",
        "print('##############################')\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCdyS6YY-NSS"
      },
      "source": [
        "### Lower Casting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m14s7kx-NST"
      },
      "source": [
        "Converting a word to lower case (NLP -> nlp). Words like Book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimensions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1ONctSi-NST",
        "outputId": "7b6610e6-bf1b-4a33-ed2e-5a2f74d721b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"i'm enjoying the nlp course!\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I'm enjoying the NLP course!\"\n",
        "text = text.lower()\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qmaJLl1-NSU"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFV5GPbs-NSU"
      },
      "source": [
        "Stemming is basically removing the suffix from a word and reduce it to its root word. For example: “Flying” is a word and its suffix is “ing”, if we remove “ing” from “Flying” then we will get base word or root word which is “Fly”. We uses these suffix to create a new word from original stem word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLlwCwbR-NSU"
      },
      "source": [
        "https://www.nltk.org/howto/stem.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYq5DhuA-NSe"
      },
      "source": [
        "#### What is PorterStemmer?\n",
        "\n",
        "It is one of the most popular stemming methods proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. This stemmer is known for its speed and simplicity. The main applications of Porter Stemmer include data mining and Information retrieval. However, its applications are only limited to English words. Also, the group of stems is mapped on to the same stem and the output stem is not necessarily a meaningful word. The algorithms are fairly lengthy in nature and are known to be the oldest stemmer.\n",
        "\n",
        "> Advantage: It produces the best output as compared to other stemmers and it has less error rate.\n",
        "\n",
        "> Limitation:  Morphological variants produced are not always real words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h35Nl_k-NSf",
        "outputId": "bb93e933-dc99-49e2-99a1-c2508eac6db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fairli\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "\n",
        "for word in words:\n",
        "    print(word + ' --> ' + ps.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asv0B09E-NSf"
      },
      "source": [
        "#### What is SnowballStemmer?\n",
        "\n",
        "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\n",
        "\n",
        "> Advantage: It is slightly faster computation time than porter, with a reasonably large community around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mghWHdFQ-NSg",
        "outputId": "30f30009-8055-4f22-f758-121dbae9ac4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fair\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "sn = SnowballStemmer(language='english')\n",
        "\n",
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "\n",
        "for word in words:\n",
        "    print(word+' --> '+sn.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffHvTpzw-NSg"
      },
      "source": [
        "#### Comparing between porter and snowball"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEGSNcRD-NSh",
        "outputId": "2961d3cf-2ec4-427f-de67-918c83df3e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generous --> gener\n",
            "generous --> generous\n",
            "---------------------------------------\n",
            "generation --> gener\n",
            "generation --> generat\n",
            "---------------------------------------\n",
            "generously --> gener\n",
            "generously --> generous\n",
            "---------------------------------------\n",
            "generate --> gener\n",
            "generate --> generat\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "words = ['generous','generation','generously','generate']\n",
        "\n",
        "for word in words:\n",
        "    print(word+' --> '+ps.stem(word))\n",
        "    print(word+' --> '+sn.stem(word))\n",
        "    print('---------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc9hKyR_-NSh"
      },
      "source": [
        "**Note**: Spacy does not provide stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxJ-MQoN-NSi"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG8oY3pn-NSi"
      },
      "source": [
        "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word.\n",
        "\n",
        "> One major difference with stemming is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTx6qQBH-NSj",
        "outputId": "a419e089-d95f-4fef-89e6-f217a9162f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I:  I\n",
            "am:  am\n",
            "enjoying:  enjoying\n",
            "with:  with\n",
            "AI:  AI\n",
            "courses:  course\n",
            ".:  .\n",
            "Dr.Adam:  Dr.Adam\n",
            "is:  is\n",
            "teaching:  teaching\n",
            "me:  me\n",
            "two:  two\n",
            "courses:  course\n",
            ".:  .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "text = 'I am enjoying with AI courses. Dr.Adam is teaching me two courses.'\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for word in nltk.word_tokenize(text):\n",
        "    print(f\"{word}: \", lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzqto9z_-NSk"
      },
      "source": [
        "You may go through Spacy's lemmatizer: https://spacy.io/api/lemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8yYChEv-NSk"
      },
      "source": [
        "### ٍStop Words Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtCHe1pX-NSl"
      },
      "source": [
        "The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiSHkBs6-NSl"
      },
      "source": [
        "#### Why do we need to remove stopwords?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rsVzXnI-NSm"
      },
      "source": [
        "By removing these words, we remove the low-level information from our text in order to give more focus to the important information. In order words, we can say that the removal of such words does not show any negative consequences on the model we train for our task.\n",
        "\n",
        "- reduces the dataset size\n",
        "- reduces the training time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdyy5e-9-NSm"
      },
      "source": [
        "#### Do we always remove stopwords? **NO!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erz9jaK7-NSn"
      },
      "source": [
        "We do not always remove the stop words. The removal of stop words is highly dependent on the task we are performing and the goal we want to achieve. For example, if we are training a model that can perform the sentiment analysis task, we might not remove the stop words.\n",
        "\n",
        "Movie review: “The movie was not good at all.”\n",
        "\n",
        "Text after removal of stop words: “movie good”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny55MORZ-NSo"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjFF266q-NSo",
        "outputId": "ef3d3b48-4938-40c8-a305-32257816d93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'few', 'once', 'whence', 'them', 'been', 'seemed', 'him', 'which', 'front', 'myself', 'we', 'otherwise', 'formerly', 'down', 'whether', 'has', 'say', 'to', 'his', 'across', 'anyway', 'no', 'everyone', 'throughout', '‘ll', 'therein', 'for', 'sometimes', 'else', 'latterly', 'nowhere', 'together', 'first', 'above', 'becoming', 'less', \"'ll\", '‘d', 'everything', 'out', 'there', 'various', 'via', 'our', 'before', 'go', 'it', 'fifty', 'third', 'were', 'former', 'made', 'as', 'eleven', 'toward', 'mine', \"'m\", 'then', 'except', 'whereas', 'now', 'but', 'becomes', 'alone', 'more', 'although', 'get', 'three', 'nevertheless', 'ten', 'without', 'yours', 'thereupon', 'on', 'all', 'whom', 'mostly', 'than', 'sometime', 'had', 'call', 'yourself', 'she', 'between', 'why', 'of', 'much', 'side', 'yet', 'give', 'some', 'until', 'somewhere', 'may', 'after', 'move', 'among', 'doing', 'nobody', 'rather', 'towards', 'twenty', 'however', 'beyond', '’m', 'same', 'each', 'the', 'whole', 'make', 'have', 'nor', 'onto', 'n‘t', 'whose', 'off', 'five', 'along', 'while', 'hundred', 'again', 'they', 'around', 'both', 'he', 'themselves', 'really', 'done', 'that', 'elsewhere', \"'d\", 'noone', 'an', 'never', 'thereby', 'must', '‘m', 'over', 'your', 'meanwhile', 'whenever', 'be', 'about', 'into', 'is', '’ve', 'whither', 'herein', 'several', 'ever', 'namely', 'whereafter', 'quite', 'most', 'being', 'last', 'where', 'in', 'its', 'please', 'by', '‘ve', 'did', 'whereupon', 'afterwards', 'fifteen', 'somehow', 'when', 'very', 'these', 'other', 'even', 'bottom', 're', 'moreover', 'wherein', 'many', 'hereupon', 'you', 'further', 'up', 'per', 'regarding', 'cannot', 'would', 'are', '’re', 'twelve', 'anything', 'could', 'everywhere', 'using', 'not', 'against', 'always', 'take', 'though', 'seems', 'perhaps', 'back', 'due', 'do', '‘re', 'own', '’d', 'often', 'beside', 'nothing', 'during', 'hereby', 'whoever', 'since', 'those', 'thence', 'yourselves', 'used', 'full', 'show', 'can', 'so', 'was', 'therefore', 'any', 'ours', 'below', 'who', 'me', 'least', 'here', 'anyhow', 'nine', 'next', 'wherever', 'empty', 'within', 'eight', \"'s\", 'or', 'from', 'whatever', 'anyone', \"'ve\", \"n't\", 'either', 'four', 'how', 'their', 'put', 'too', 'every', 'whereby', 'still', '‘s', 'amongst', 'forty', 'one', 'my', 'am', 'only', 'well', 'become', 'because', 'none', 'see', 'seeming', 'thereafter', 'amount', 'someone', 'just', 'part', 'at', 'behind', 'this', 'others', 'through', 'unless', 'hence', 'upon', 'latter', 'already', 'two', 'anywhere', 'under', 'six', 'enough', 'became', 'us', 'besides', 'i', 'with', 'almost', 'beforehand', 'might', 'sixty', 'thus', 'seem', 'should', 'will', 'hers', 'what', 'serious', 'btw', 'name', 'herself', 'himself', 'another', '’ll', 'itself', 'hereafter', 'n’t', '’s', 'thru', 'such', 'indeed', 'keep', 'and', 'top', 'ourselves', 'if', \"'re\", 'ca', 'does', 'also', 'something', 'her', 'a', 'neither'}\n"
          ]
        }
      ],
      "source": [
        "print(nlp.Defaults.stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xReSqgca-NSp",
        "outputId": "1af8a2b8-4d05-4121-8068-b8788bd4fb91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(nlp.vocab['myself'].is_stop)\n",
        "print(nlp.vocab['mystery'].is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekOKJmQ5-NSq",
        "outputId": "a34c88d4-48a3-4318-ed59-3f18e814aef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "nlp.Defaults.stop_words.add('btw')\n",
        "print (nlp.vocab['btw'].is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkMOTX5g-NSq"
      },
      "outputs": [],
      "source": [
        "nlp.Defaults.stop_words.remove('beyond')\n",
        "nlp.vocab['beyond'].is_stop = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ti3ageY-NSr",
        "outputId": "56163da7-0918-4fcd-a06e-27346d00572e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775nIAVr-NSr",
        "outputId": "897376c5-b5de-4f6d-ef67-1aaeb3352c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "179\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(len(stop_words))\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp8R4xAGegjk"
      },
      "source": [
        "_________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0h04Fmq-NSs"
      },
      "source": [
        "# Text pre-processing Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87mzleCM-NSs"
      },
      "source": [
        "To make you understand better about the need of text pre-processing. We will do a small project. In this project, I need to find how many hashtags and the top 10 hashtags used in the dataset.\n",
        "\n",
        "The dataset is about people tweets about apple company in the twitter. You can find the dataset here: https://www.kaggle.com/datasets/seriousran/appletwittersentimenttexts\n",
        "\n",
        "Output labels:\n",
        "\n",
        "- -1: negative\n",
        "- 0: neutral\n",
        "- 1: positive\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
